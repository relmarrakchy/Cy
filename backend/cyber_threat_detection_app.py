# -*- coding: utf-8 -*-
"""Cyber threat detection app.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13G9eLWvnaHhTPBlirOsuqvYosSxGuEUW

# Cyber threat detecetion notebook - Author : ELMARRAKCHY Reda


---

> In today's interconnected digital world, cyber threats pose significant risks to individuals, organizations, and even nations. Cyber threat detection is the practice of identifying and mitigating potential cyber threats, including malware, hacking attempts, phishing attacks, and other malicious activities, before they can cause harm.

In this Python notebook, we'll try to make a cyber threat detection model using Python libraries such as pandas, scikit-learn, and TensorFlow. Let's dive in!

## I - Importing libraries and loading the dataset :

Importing libraries :
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import pickle

"""Loading the dataset :"""

dataset = pd.read_csv("./cyberthreat_dataset.csv")

dataset.head()

"""## II - Discovering our dataset :"""

dataset.info()

"""It appears that our dataset contains information regarding previous cyber attacks on an organizational network. To enhance our ability to detect cyber threats, I will focus on utilizing the following features: "Protocol", "Flag", "Packet", "Sender ID", "Source IP Address", "Source Port", "Packet Size", and "Target Variable". These features are considered relevant and are expected to contribute significantly to our cyber threat detection efforts."""

dataset = dataset[["Protocol", "Flag", "Packet", "Sender ID", "Source IP Address", "Source Port", "Packet Size", "Target Variable"]]

dataset.info()

"""It's noted that there are no missing values in our dataset.

Let's make some visualizations to get more insights about our dataset :
"""

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
sns.countplot(data=dataset, x="Protocol", hue="Flag")
plt.title('Protocols')

plt.subplot(1, 2, 2)
sns.countplot(data=dataset, x="Packet", hue="Flag")
plt.title('Packets')

plt.tight_layout()

plt.show()

"""*   The packets are using UDP protocol : **DHCP, DNS, SNMP, NTP**
*   The packets are using TCP protocol : **FTP, HTTPS, HTTP, SMTP, SSH**

*  **FIN** flag means the connection termination and only send by **HTTPS** packets.
*  **RST** flag means the immediate connection termination caused by unrecoverable errors, and only send by **FTP** packets.
*  **SYN** flag means the connection initialization, and send by **FTP**, **HTTP**, **SMTP** and SSH packets
"""

sns.countplot(data=dataset, x="Target Variable", hue="Protocol")
plt.title('Target Variable depends on protocols')
plt.xticks(rotation=90)

plt.show()

def make_pie_data(dataset, feature_1, feature_2):
    grouped_target = dataset.groupby(feature_1)
    grouped_packet = dataset.groupby(feature_2)
    targets = list(grouped_target.groups.keys())
    packets = list(grouped_packet.groups.keys())

    res = {}

    for t in targets:
        total_size = len(dataset[dataset[feature_1] == t])
        res[t] = {}

        for p in packets:
            partial_size = len(dataset[(dataset[feature_1] == t) & (dataset[feature_2] == p)])

            if partial_size != 0:
                res[t][p] = (total_size / 100) * partial_size

    return res

def draw_pie_plot(data):
  plt.figure(figsize=(18, 10))

  i = 1
  for target in data:
    packets = []
    rates = []
    for packet in data[target]:
      packets.append(packet)
      rates.append(data[target][packet])
    colors = sns.color_palette('pastel')[0:len(packets)]
    plt.subplot(3, 3, i)
    plt.pie(rates, labels = packets, colors = colors, autopct='%.0f%%')
    plt.title(f"{target}")
    i += 1
  plt.tight_layout()
  plt.show()

def draw_pie_plot_ports(data):
  plt.figure(figsize=(18, 15))

  i = 1
  for target in data:
    packets = []
    rates = []
    for packet in data[target]:
      packets.append(packet)
      rates.append(data[target][packet])
    colors = sns.color_palette('pastel')[0:len(packets)]
    plt.subplot(5, 5, i)
    plt.pie(rates, labels = packets, colors = colors, autopct='%.0f%%')
    plt.title(f"{target}")
    i += 1
  plt.tight_layout()
  plt.show()

data = make_pie_data(dataset, "Target Variable", "Packet")
draw_pie_plot(data)

data = make_pie_data(dataset, "Packet", "Target Variable")
draw_pie_plot(data)

data = make_pie_data(dataset, "Source Port", "Target Variable")
draw_pie_plot_ports(data)

data = make_pie_data(dataset, "Source IP Address", "Target Variable")
draw_pie_plot_ports(data)

sns.countplot(data=dataset, x="Packet Size", hue="Target Variable")
plt.show()

"""It appears that all the features are categorical data. Let's convert them."""

cols = dataset.columns

for col in cols:
  dataset[col] = dataset[col].astype("category")

dataset.info()

"""## III - Data preprocessing :

Importing the necessary libraries:
"""

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

"""Let's convert our categorical data into numerical values."""

encoders = []
for col in cols[:-1]:
  encoder = LabelEncoder()
  dataset[col] = encoder.fit_transform(dataset[col])
  encoders.append(encoder)

dataset.head()

"""Extracting the features and the target:"""

X = dataset.iloc[:, :-1]
y = dataset.iloc[:, -1]

encoder_y = LabelEncoder()
y = encoder_y.fit_transform(y)

"""Splitting our dataset into a training set and a test set:"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = True)

"""## IV - The model :

Importing the necessary libraries:
"""

from sklearn.neighbors import KNeighborsClassifier

"""Our model is a neural network model with four dense layers: **two hidden layers** with **ReLU activation** functions, and an **output layer** with **softmax activation**. Our model is compiled with the **Adam optimizer**, **sparse categorical cross-entropy** loss, and **accuracy** as **the evaluation metric**. The input shape indicating our seven features."""

model = KNeighborsClassifier(n_neighbors=5)

"""## V - Training the model :"""

model.fit(X_train, y_train)

"""## VII - Evaluation :

Importing the necessary libraries:
"""

from sklearn.metrics import (confusion_matrix, classification_report, accuracy_score)

y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)

accuracy

conf_matrix = confusion_matrix(y_test, y_pred)

# Plot the heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

print(classification_report(y_test, y_pred))

with open('/model_cyber.pkl', 'wb') as f:
    pickle.dump(model, f)

with open('pickels/label_encoder_cyber_f.pkl', 'wb') as f:
    pickle.dump(encoders, f)

with open('pickels/label_encoder_cyber_y.pkl', 'wb') as f:
    pickle.dump(encoder_y, f)